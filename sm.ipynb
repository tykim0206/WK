{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-00bcf6b7633c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTwitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKkma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "import csv\n",
    "import codecs\n",
    "import seaborn\n",
    "import numpy\n",
    "import operator\n",
    "import pandas\n",
    "import seaborn\n",
    "\n",
    "from konlpy.tag import Twitter, Kkma, Komoran, Hannanum\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url ='http://news.naver.com/main/search/search.nhn?query=%BD%BA%C7%C7%B5%E5%B8%DE%C0%CC%C6%AE&startDate=&endDate=&page={}'\n",
    "\n",
    "#tit_url = './/a[@class = \"go_naver\"]'\n",
    "#text_url = './/div[@id =\"articleBodyContents\"]'\n",
    "\n",
    "with open('sm.csv', 'w', encoding='utf8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for page in range(1,40):\n",
    "        res =requests.get (base_url.format(page))\n",
    "        root_list = lxml.html.fromstring(res.text)\n",
    "#        print (page)\n",
    "\n",
    "        for node in root_list.xpath('.//a[@class = \"go_naver\"]'):\n",
    "            try:\n",
    "                res = requests.get (node.attrib['href'])\n",
    "                sub_list = lxml.html.fromstring(res.text)\n",
    "#                print (node.text_content(), node.attrib['href'])\n",
    "                sub_text =sub_list.xpath( './/div[@id =\"articleBodyContents\"]')[0] \n",
    "#                print (sub_text.text_content())\n",
    "                writer.writerow([sub_text.text_content()])            \n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exclude = exclude = ['뉴스','뉴시스','무단','배포','정비','자동차','메이트','네트웍','서비스','스피드','고객','기자','관계자','이데일리','전국',\n",
    "                            '경우','이번','위해','하이','현재','한국','통해','전재','대표','부문','사업']\n",
    "\n",
    "def get_word (doc) :    #1음절, 특정단어 삭제\n",
    "    nouns = tagger.nouns(doc)\n",
    "    return [noun for noun in nouns if len(noun)>1 and noun not in exclude ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = []\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "with open('sm.csv', encoding='utf8', newline='\\r\\n') as f:\n",
    "    reader = csv.reader(f)\n",
    "    \n",
    "    for row in reader:\n",
    "        news.append(row[0])\n",
    "\n",
    "    tagger = Twitter()\n",
    "    cv = CountVectorizer(tokenizer=get_word, max_features=50)\n",
    "    tdf = cv.fit_transform(news)\n",
    "    words = cv.get_feature_names()\n",
    "    count_mat = tdf.sum(axis=0)\n",
    "    count = numpy.squeeze(numpy.asarray(count_mat))\n",
    "    word_count = list(zip(words, count))\n",
    "       \n",
    "    wc = WordCloud(font_path='C:\\\\Windows\\\\Fonts\\\\malgun.ttf', background_color='white', width=500, height=400)\n",
    "    cloud = wc.generate_from_frequencies(word_count)\n",
    "    \n",
    "    pyplot.figure(figsize=(12,9))\n",
    "    pyplot.imshow(cloud)\n",
    "    pyplot.axis(\"off\")\n",
    "    pyplot.show()\n",
    "   \n",
    "#    print (year, word_count)\n",
    "    \n",
    "#    with open('tot_tdm1.csv', 'a', encoding='utf8') as f:\n",
    "#        f.write('\\n'.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
